---
title: "Hyperparameter Tuning"
author: "R. Dimas Bagas Herlambang"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
    df_print: paged
---

```{r setup, include=FALSE}
# clear-up the environment
rm(list = ls())

# chunk options
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  fig.asp = 0.5625,
  fig.align = "center",
  out.width = "85%",
  comment = "#>"
)

# import libs
library(plotly)
library(tidyverse)
library(tidymodels)
library(xgboost)
```

## Data Preparation

### Import Dataset

```{r data-import}
# import dataset
data_clean <- read_csv("data/data-clean.csv")

# quick check
head(data_clean, 10)
```

## Data Preprocess

### Initial Split

```{r preproc-initial_split}
# set seed
set.seed(100)

# create initial split
splitted <- initial_split(data_clean, prop = 0.8, strata = "attrition")

# quick check
splitted
```

### Defining Preprocess Recipe

```{r preproc-rec}
# define preprocess recipe from train dataset
rec <- recipe(attrition ~ ., data = training(splitted)) %>% 
  step_rm(employee_count, employee_number) %>%
  step_nzv(all_predictors()) %>% 
  step_string2factor(all_nominal(), -attrition) %>%
  step_string2factor(attrition, levels = c("yes", "no")) %>%
  step_downsample(attrition, ratio = 1/1, seed = 100) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_pca(all_numeric(), threshold = 0.85) %>% 
  step_dummy(all_predictors(), -all_numeric()) %>% 
  prep(strings_as_factors = FALSE)

# get train and test dataset
data_train <- juice(rec)
data_test <- bake(rec, testing(splitted))

# quick check
head(data_train, 10)
```

### Cross-Validation Scheme

```{r preproc-vfold_cv}
# set seed
set.seed(100)

# create cv split
cv_split <- vfold_cv(data_train, v = 3, repeats = 2, strata = "attrition")

# quick check
cv_split
```

## Model Fitting

### Defining Model Specifications

```{r model-engine}
# define model specification
model_spec <- boost_tree(
  mode = "classification",
  loss_reduction = 0.01,
  learn_rate = 0.01,
  sample_size = 0.8
)

# define model engine
model_engine <- set_engine(
  object = model_spec,
  engine = "xgboost",
  nthread = parallel::detectCores() / 2
)

# quick check
model_engine
```

### Defining Parameter Grid

```{r model-grid}
# set-up model grid
model_grid <- grid_regular(
  range_set(mtry, range = c(2, ncol(data_train) - 2)),
  range_set(trees, range = c(500, 1500)),
  range_set(min_n, range = c(1, 30)),
  range_set(tree_depth, range = c(1, 8)),
  levels = 3
)

# quick check
model_grid
```

### Model Fitting

```{r model-fit}
# merge model engine and grid
model_specs <- tibble(spec = merge(model_engine, model_grid)) %>%
  mutate(spec_id = str_pad(row_number(), width = 2, side = "left", pad = "0"))

# give every spec in model grid an id
model_grid <- model_grid %>%
  mutate(spec_id = str_pad(row_number(), width = 2, side = "left", pad = "0"))

# cross cv splits and model specs
crossed <- crossing(cv_split, model_specs)

# fit on every folds
crossed <- crossed %>%
  mutate(model = map2(spec, splits, ~
    fit_xy(
      object = .x,
      x = select(analysis(.y), -attrition),
      y = select(analysis(.y), attrition))
  ))

# quick check
head(crossed, 10)
```

## Cross-Validation Results

### Get Prediction on Hold-out Sample

```{r results-predict}
# get hold-out performance in every folds
cv_results <- crossed %>%
  mutate(performance = map2(model, splits, ~
    select(assessment(.y), attrition) %>%
      bind_cols(predict(.x, select(assessment(.y), -attrition))) %>%
      summarise(
        sensitivity = sens_vec(attrition, .pred_class),
        precision = precision_vec(attrition, .pred_class)
      )
  ))

# unnest the cv result
cv_results <- cv_results %>%
  select(spec_id, id, id2, performance) %>%
  unnest(performance)

# join with model grid
cv_results <- cv_results %>% 
  left_join(model_grid)

# quick check
head(cv_results, 10)
```

### Sensitivity

```{r results-sensitivity}
# plot the result
ggplot(cv_results, aes(x = as.factor(mtry), y = sensitivity)) +
  geom_boxplot(aes(fill = as.factor(trees))) +
  coord_flip() +
  facet_wrap(~ min_n + tree_depth, ncol = 3, labeller = "label_both") +
  labs(x = "mtry", y = "Sensitivity", fill = "trees") +
  theme_minimal()
```

### Precision

```{r results-precision}
# plot the result
ggplot(cv_results, aes(x = as.factor(mtry), y = precision)) +
  geom_boxplot(aes(fill = as.factor(trees))) +
  coord_flip() +
  facet_wrap(~ min_n + tree_depth, ncol = 3, labeller = "label_both") +
  labs(x = "mtry", y = "Precision", fill = "trees") +
  theme_minimal()
```

## Best Model Selection

```{r best-get}
# get best model by sensitivity
best_model <- cv_results %>% 
  filter(sensitivity < 1, sensitivity > 0) %>% 
  group_by(spec_id) %>% 
  summarise(
    mean = mean(sensitivity),
    sd = sd(sensitivity)
  ) %>% 
  ungroup() %>% 
  mutate(
    score_mean = round((mean - min(mean)) / (max(mean) - min(mean)), 4),
    score_sd = 1 - round((sd - min(sd)) / (max(sd) - min(sd)), 4),
    score_total = 0.5 * score_mean + 0.5 * score_sd
  ) %>% 
  arrange(desc(score_total)) %>% 
  dplyr::slice(1)

# check best model specification
left_join(best_model, model_grid)
```

```{r best-fit}
# get best model specification
best_model_spec <- best_model %>% 
  left_join(model_specs) %>% 
  pull(spec) %>% 
  pluck(1)

# fit the model
model <- fit_xy(
  object = best_model_spec,
  x = select(data_train, -attrition),
  y = select(data_train, attrition)
)

# quick check
model
```

### Variable Importance

```{r model-var_imp}
# get variable importance
var_imp <- xgb.importance(names(select(data_train, -attrition)), model$fit)

# tidying
var_imp <- var_imp %>%
  head(10) %>% 
  rename(variable = Feature, importance = Gain) %>%
  mutate(variable = reorder(variable, importance))

# variable importance plot
ggplot(var_imp, aes(x = variable, y = importance)) +
  geom_col(fill = "darkblue") +
  coord_flip() +
  labs(title = "Variables Importance (Top 10)", x = NULL, y = NULL, fill = NULL) +
  scale_y_continuous(expand = expand_scale(mult = c(0, 0.1))) +
  theme_minimal()
```

## Best Model Evaluation

### Predict on Test Dataset

```{r eval-pred}
# predict on test
pred_test <- select(data_test, attrition) %>%
  bind_cols(predict(model, select(data_test, -attrition))) %>%
  bind_cols(predict(model, select(data_test, -attrition), type = "prob"))

# quick check
head(pred_test, 10)
```

### Confusion Matrix

```{r eval-conf_mat}
# confusion matrix
pred_test %>%
  conf_mat(attrition, .pred_class) %>%
  autoplot(type = "heatmap")
```

```{r eval-conf_mat-summary}
# metrics summary
pred_test %>%
  summarise(
    accuracy = accuracy_vec(attrition, .pred_class),
    sensitivity = sens_vec(attrition, .pred_class),
    specificity = spec_vec(attrition, .pred_class),
    precision = precision_vec(attrition, .pred_class)
  )
```

### ROC Curve

```{r eval-roc_curve}
# plot roc curve
pred_test %>%
  roc_curve(attrition, .pred_yes) %>%
  autoplot()
```

```{r eval-roc_curve-trade_off}
# get roc curve data on test dataset
pred_test_roc <- pred_test %>%
  roc_curve(attrition, .pred_yes)

# tidying
pred_test_roc <- pred_test_roc %>% 
  mutate_if(~ is.numeric(.), ~ round(., 4)) %>% 
  gather(metric, value, -.threshold)

# plot sensitivity-specificity trade-off
p <- ggplot(pred_test_roc, aes(x = .threshold, y = value)) +
  geom_line(aes(colour = metric)) +
  labs(x = "Probability Threshold to be Classified as Positive", y = "Value", colour = "Metrics") +
  theme_minimal()

ggplotly(p)
```

### Precision-Recall Curve

```{r eval-pr_curve}
# plot pr curve
pred_test %>%
  pr_curve(attrition, .pred_yes) %>%
  autoplot()
```

```{r eval-pr_curve_trade_off}
# get pr curve data on test dataset
pred_test_pr <- pred_test %>%
  pr_curve(attrition, .pred_yes)

# tidying
pred_test_pr <- pred_test_pr %>% 
  mutate_if(~ is.numeric(.), ~ round(., 4)) %>% 
  gather(metric, value, -.threshold)

# plot recall-precision trade-off
p <- ggplot(pred_test_pr, aes(x = .threshold, y = value)) +
  geom_line(aes(colour = metric)) +
  labs(x = "Probability Threshold to be Classified as Positive", y = "Value", colour = "Metrics") +
  theme_minimal()

ggplotly(p)
```
