---
title: "Model Fitting and Evaluation Report"
author: "R. Dimas Bagas Herlambang"
date: "`r format(Sys.Date(), '%B %e, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: 
      collapsed: false
    number_sections: true
    code_folding: hide
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.asp = 0.65,
  fig.align = "center",
  out.width = "85%",
  collapse = TRUE,
  comment = "#>"
)
```

```{r libs, include=FALSE}
library(dials)
library(ranger)
library(tidyverse)
library(tidymodels)
```

# Dataset

```{r}
# import dataset
data_train <- read_csv("data/data-train.csv")

# quick check
head(data_train, 10)
```

# Data Preprocess and Cross-Validation Scenario

## Initial Split

```{r}
# set seed
set.seed(100)

# create initial split scheme
splitted <- initial_split(data_train, prop = 0.8, strata = "y")

# quick check
head(training(splitted), 10)
```

## Set-up Preprocess Recipe

```{r}
# create a recipe based on training data
rec <- recipe(y ~ ., data = training(splitted)) %>%
  step_rm(cust_id, call_date) %>%
  step_string2factor(all_nominal(), -all_outcomes()) %>%
  step_string2factor(y, levels = c("yes", "no")) %>%
  step_downsample(y, seed = 100) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  prep(strings_as_factors = FALSE)

# quick check
head(juice(rec), 10)
```

## Set-up Cross Validation Scenario

```{r}
# set seed
set.seed(100)

# create cv split
cv_split <- vfold_cv(juice(rec), v = 3, repeats = 2, strata = "y")

# quick check
cv_split
```

```{r}
# quick check: first train data
analysis(cv_split$splits[[1]])
```

```{r}
# quick check: first test data
assessment(cv_split$splits[[1]])
```

# Set-up Models

## Define Model and Parameter Grid

### Model Engine

```{r}
# set-up a model engine
model_engine <- rand_forest(mode = "classification") %>%
  set_engine(
    engine = "ranger",
    seed = 100,
    num.threads = parallel::detectCores(),
    importance = "impurity"
  )

# quick check
model_engine
```

### Parameter Grid

```{r}
# set-up model grid
model_grid <- grid_regular(
  range_set(mtry, range = c(2, ncol(juice(rec)) - 1)),
  range_set(trees, range = c(500, 500)),
  range_set(min_n, range = c(1, 10)),
  levels = 3
)

# quick check
model_grid
```

### All Models

```{r}
# merge model engine and grid
model_specs <- tibble(spec = merge(model_engine, model_grid)) %>%
  mutate(spec_id = str_pad(row_number(), width = 2, side = "left", pad = "0"))

# quick check
model_specs
```

## All Cross-Validation and Models Combination

```{r}
# cross cv splits and model specs
crossed <- crossing(cv_split, model_specs)

# quick check
crossed
```

# Model Fitting and Evaluation

## Get the Cross Validation Result

```{r}
# fit on every folds
crossed <- crossed %>%
  mutate(model = map2(spec, splits, ~
    fit_xy(.x, x = select(analysis(.y), -y), y = select(analysis(.y), y))
  ))

# get hold-out prediction in every folds
cv_result <- crossed %>%
  mutate(prediction = map2(model, splits, ~
    assessment(.y) %>%
    bind_cols(predict(.x, new_data = .)) %>%
    bind_cols(predict(.x, new_data = ., type = "prob")) %>%
    select(y, starts_with(".pred"))
  ))

# unnest the cv result
cv_result <- cv_result %>%
  select(id, id2, spec_id, prediction) %>%
  unnest(prediction)

# give every spec in model grid an id
model_grid <- model_grid %>%
  mutate(spec_id = str_pad(row_number(), width = 2, side = "left", pad = "0"))

# combine with cv result
cv_result <- cv_result %>%
  left_join(model_grid)

# quick check
cv_result
```

## Analyze the Cross-Validation Results

```{r}
# get metrics for every model
grid_result <- cv_result %>%
  group_by(mtry, trees, min_n) %>%
  summarise(
    accuracy = accuracy_vec(y, .pred_class),
    recall = recall_vec(y, .pred_class),
    precision = precision_vec(y, .pred_class)
  ) %>%
  ungroup() %>%
  gather(metrics, value, accuracy:precision)

# plot the result
ggplot(grid_result, aes(x = mtry, y = value)) +
  geom_point() +
  geom_line() +
  facet_wrap(~ metrics + min_n, ncol = 3, labeller = "label_both") +
  labs(y = "Metrics Value", colour = "trees") +
  theme_minimal()
```

# Final Model

## Get Final Model

```{r}
# filter to model with best performance across all folds
best_model <- cv_result %>%
  group_by(spec_id, id, id2) %>%
  summarise(recall = recall_vec(y, .pred_class)) %>%
  group_by(spec_id) %>%
  summarise(recall = mean(recall)) %>%
  ungroup() %>%
  arrange(desc(recall)) %>%
  head(1)

# check best model specification
best_model %>%
  left_join(model_grid)
```

## Fit the Final Model on Full Train Dataset

```{r}
# get the best model spec
model_spec <- crossed %>%
  right_join(best_model) %>%
  distinct(spec_id, .keep_all = TRUE) %>%
  pull(spec) %>%
  pluck(1)

# fit on train
model <- fit_xy(model_spec, x = juice(rec, -y), y = juice(rec, y))

# quick check
model
```

## Predict on Test Dataset

```{r}
# predict on test
pred_test <- bake(rec, testing(splitted)) %>%
  bind_cols(predict(model, new_data = .)) %>%
  bind_cols(predict(model, new_data = ., type = "prob"))

# quick check
pred_test
```

## Confusion Matrix

```{r}
# confusion matrix
pred_test %>%
  conf_mat(y, .pred_class) %>%
  autoplot(type = "heatmap")
```

## Model Performance Metrics

```{r}
# metrics summary
pred_test %>%
  summarise(
    accuracy = accuracy_vec(y, .pred_class),
    recall = recall_vec(y, .pred_class),
    precision = precision_vec(y, .pred_class),
    roc_auc = roc_auc_vec(y, .pred_yes)
  )
```

## ROC Curve

```{r}
# roc curve
pred_test %>%
  roc_curve(y, .pred_yes) %>%
  autoplot()
```
